## **1. Where Things Start to Suck**  
### **Why Communication Between Microservices Is Harder Than It Looks**

Building microservices? Migrating from a monolith? Doesn‚Äôt matter. The moment you have more than one service owning data, communication becomes the real beast.

Your neat service boundaries don‚Äôt mean much if Service A needs data from Service B and the whole thing grinds to a halt because one endpoint is flaky. That local `function()` call you took for granted? It‚Äôs now a network call‚Äîslow, unreliable, and full of edge cases.

This is where most systems break. Not at ‚Äúscale.‚Äù Not at ‚Äúmachine learning.‚Äù At **"how the hell do I get this data over there?"**

This article walks through how data moves between services:
- What people do in the beginning (and regret later)
- How async messaging and event-driven design actually help
- When to use replication and CDC without shooting yourself in the foot
- How not to lose events and keep your sanity

No fluff. Just patterns, trade-offs.


---

## **2. Shared Database: The Classic Trap**

At some point, someone decides:  
> ‚ÄúWhy don‚Äôt we just let all the services access the same database? It‚Äôs faster and we don‚Äôt have to deal with APIs.‚Äù

Sounds smart. It‚Äôs not.

Letting multiple services hit the same physical database looks easy‚Äîuntil it turns your system into a giant, unversioned spaghetti monster. You‚Äôve just killed any hope of proper ownership, encapsulation, or independent deployability.

![Shared Database](./images/shared_db.jpg)

### Why This Happens
- It‚Äôs fast to build.
- No need to design APIs.
- Feels like the ‚Äúmonolith, but modular.‚Äù

### Why It Blows Up
- **Tight coupling**: One service changes the schema, another breaks.
- **No autonomy**: Can‚Äôt scale or deploy independently.
- **No boundaries**: Everyone‚Äôs reading and writing whatever they want.
- **Hidden dependencies**: No one knows who depends on what anymore.
- **Zero traceability**: Good luck debugging which service did what.

> You didn‚Äôt build microservices. You built a **distributed monolith**. A shared database is like everyone sharing one toothbrush‚Äîcheap, quick, and disgusting in the long run.

### ‚ö†Ô∏è If You Absolutely Have To (But You Shouldn't)

Alright. Let‚Äôs say you‚Äôre in a fire-fighting phase, a team needs data fast, and you‚Äôve got zero bandwidth to design clean APIs. Fine. Here‚Äôs how to keep the damage contained:

- **Internal-only tooling**, low-risk, low-impact.
- **Temporary hacks** ‚Äî but write it down, flag it, and kill it in a follow-up sprint.
- **Read-only access** for BI/reporting services ‚Äî isolate it to a read replica if you‚Äôre smart.

Even then, you‚Äôre skating on thin ice.

### üö´ What Not to Do (Seriously, Stop It)
- Don‚Äôt let services **write** to each other‚Äôs tables. You‚Äôre just begging for corruption.
- Don‚Äôt skip APIs because ‚Äúit‚Äôs faster.‚Äù That‚Äôs not speed‚Äîit‚Äôs debt.
- Don‚Äôt pretend this is okay ‚Äúfor now‚Äù and forget about it. That‚Äôs how systems rot.

Got it‚Äîno more bullet-to-bullet slide copying. Let‚Äôs rewrite the **Direct Service Calls** section as a proper article section‚Äînarrative, sharp, and in your style.

---

## **3. Direct Service Calls (Point-to-Point)**  
### *Real-Time, Real Fast, Real Fragile*

Once your services have their own data, they need to talk. The instinctive move?  
**‚ÄúJust call the other service.‚Äù**

![Direct Service Code](./images/direct_call.jpg)

It‚Äôs simple. Service A hits Service B over HTTP or gRPC. You get a response, update your flow, done. This feels just like calling a local function‚Äîonly it‚Äôs going over the network now.

And that‚Äôs where it starts to hurt.

Direct calls introduce **runtime coupling**. If Service B is slow, A is slow. If B is down, A is probably broken too. Chain a few of these together, and one bad apple can take out the whole orchard.

This pattern is natural. But if you don‚Äôt design for failure, you‚Äôre setting yourself up for pain.

---

### What You‚Äôre Getting Into

The good:
- You get results **right now**. No need to wait for events or build extra read models.
- Debugging is straightforward. You know who made the call and what it got back.
- Teams don‚Äôt have to learn new mental models‚Äîjust build and call APIs.

The bad:
- **Every service is now part of your availability story.**
- Latency adds up fast. A ‚Üí B ‚Üí C ‚Üí D ‚Üí timeout.
- One slow call and suddenly users are blaming your UI.
- It pushes you toward a distributed monolith if left unchecked.

### Use It Like a Grown-Up

> One direct call? Fine. Five chained calls? You‚Äôve built a Rube Goldberg machine with a timeout problem.

Direct calls **aren‚Äôt wrong**. They‚Äôre essential in places where consistency and timing matter‚Äîlike confirming a payment or validating an action before it moves forward.

But you need to **wrap them in protection**:
- Use **service discovery** so you‚Äôre not hardcoding IPs like it‚Äôs 2000s.
- Implement **timeouts** and **circuit breakers**‚Äîdon‚Äôt rely on hope.
- Consider a **service mesh** (like Istio, Linkerd, or Consul) to offload retries, mTLS, and observability.
- And for the love of uptime, don‚Äôt chain too many sync calls together.

Use this pattern when the trade-off is worth it. But design like **the other service will go down at the worst time**‚Äîbecause it will.

---

## **4. Asynchronous Messaging**  
### *Loose Coupling, Eventual Thinking*

Sometimes a service doesn‚Äôt need an answer‚Äîit just needs to tell the world something happened. That‚Äôs where **asynchronous messaging** fits in.

Instead of making a synchronous call and waiting for a response, a service emits an event‚Äîlike `OrderCreated` or `InvoicePaid`. It pushes that to a message broker (Kafka, RabbitMQ, NATS, etc.), and other services subscribe to those events and react in their own time.

It‚Äôs not better or worse than a direct call‚Äîit‚Äôs a **different shape of communication**. Use it when the need fits.

![Event Driven Architecture](./images/eda.jpg)

---

### What It Enables

Asynchronous messaging allows services to communicate without depending on each other's uptime or immediate availability. It removes the tight runtime coupling that comes with direct calls. You can have multiple consumers listening to the same event, reacting differently, scaling separately, and even being deployed independently. One event can drive fan-out workflows like notifications, fulfillment, or analytics‚Äîall without the sender knowing or caring who‚Äôs listening.

It also improves system resilience. If a subscriber is down, it can catch up later. If you need to onboard a new consumer, you don‚Äôt need to touch the publisher. And most importantly: the publisher doesn‚Äôt break just because someone downstream had a bad day.

---

### What It Demands

With that freedom comes responsibility. You lose immediate feedback‚Äîthere‚Äôs no response, no confirmation, no ‚Äúsuccess.‚Äù That means you have to build for eventual consistency, and that‚Äôs not always easy to reason about.

Observability becomes trickier. If something silently fails or lags behind, it may take a while to notice. You‚Äôll need structured logging, traceability, and solid metrics to debug async flows. Message delivery also becomes your problem: will it be at-most-once, at-least-once, or exactly-once (spoiler: the last one is a lie without a lot of pain)?

And even though services are decoupled at runtime, they still depend on the shape of the events. Schema evolution, contract testing, and clear ownership of events are just as critical here as in API-driven systems.

---

### When It Fits

Asynchronous messaging is a strong fit when you‚Äôre designing for workflow orchestration, domain integrations, or decoupling. If a service can say ‚Äúthis happened‚Äù and doesn‚Äôt care who reacts, this pattern gives you breathing room and extensibility.

It‚Äôs also the go-to model for cross-cutting concerns like logging, audit trails, background processing, or reacting to state changes in systems that aren‚Äôt owned by your service.

It‚Äôs not about replacing direct calls‚Äîit‚Äôs about using async where immediate feedback isn‚Äôt needed, and flexibility is worth more than instant answers.

You reach for async messaging when the system needs to **react**, not wait. It‚Äôs not about ‚Äúbetter‚Äù or ‚Äúfaster‚Äù‚Äîit‚Äôs about the shape of the problem.

**Use it when:**

- üîÅ **You‚Äôre building asynchronous workflows**  
  A process involves multiple steps across services‚Äîuser signs up, then gets an email, then gets their onboarding flow initialized. Each step can run independently and doesn‚Äôt need a real-time guarantee.

- üîÑ **You‚Äôre integrating across domain boundaries**  
  Service A doesn‚Äôt own the logic of Service B. Instead of calling and coupling tightly, it emits events that B can respond to on its own terms. It‚Äôs a clean contract without tight binding.

- üîì **You want decoupling without losing visibility**  
  Multiple services might care about the same event‚Äîlike `InvoicePaid`. Instead of A knowing everyone to notify, it just emits the fact, and subscribers take care of their part. This gives you extensibility without rewriting the origin.


Perfect‚Äîlet‚Äôs frame the **Outbox Pattern** as a **subsection** under **Asynchronous Messaging / Event-Driven Architecture**, since that‚Äôs where it lives conceptually. Here‚Äôs how to adjust the flow:

---

### Reliability in Event-Driven Systems  
#### *The Outbox Pattern: Send Events Without Screwing Up Your Data*

Once you start emitting events, the question becomes: **how do you make sure they actually get delivered?**  
You can‚Äôt just `db.save()` and then `.publish()` and call it a day‚Äîbecause if publishing fails, your system ends up in an inconsistent state.

This is where the **Outbox Pattern** comes in.

Instead of sending events directly after a write, you write the event to an **outbox table** inside the same database transaction as your domain change. A background process (or CDC tool) later picks it up and publishes it.

This pattern guarantees that either:
- Both the data and the event get persisted, or
- Neither of them does.

It‚Äôs simple, boring, and it works.

![Outbox Pattern](./images/outbox_pattern.jpg)


- Service updates its own database (e.g., changes order status).
- It also inserts an event (e.g., `OrderPlaced`) into a dedicated `outbox_events` table in the same transaction.
- A separate publisher reads from this table and sends the events to a broker.

No risk of partial updates. No lost events. Just clean handoff.

---

#### When It‚Äôs Worth Using

- You want **reliable, consistent event delivery** without going full event sourcing.
- Your service needs to emit events as part of its core logic, not as an afterthought.
- You can‚Äôt afford dropped messages‚Äîbut don‚Äôt need two-phase commits either.

Especially useful in **transactional business domains** where correctness matters more than speed.

> Event-driven doesn‚Äôt mean ‚Äúfire and forget and hope.‚Äù  
> You need patterns like this to make sure the system holds together under pressure.

Outbox isn‚Äôt optional‚Äîit‚Äôs essential when messaging matters.

---
## 5. Event Sourcing & Event Store  
### *Don't Just Store the State ‚Äî Store the Story*

![Event Store](./images/event_store.jpg)

Most systems store the final result: ‚ÄúAccount balance is $0.‚Äù But what they don‚Äôt store is how it got there.

Event Sourcing flips that mindset. Instead of saving the latest state, you save **each business decision** that *led* to that state. You store facts: `AccountOpened`, `MoneyDeposited`, `MoneyWithdrawn`. Those events become your source of truth ‚Äî not just metadata. The final state is a consequence of applying these events in order.

You're not capturing state‚Äîyou‚Äôre capturing **intent, history, and meaning**.

---

### Modeling Business as It Actually Happens

![Event Store Data](./images/event_store_example.jpg)

When you model with events, you're not just building software ‚Äî you're representing **how the business actually operates**. A change in balance isn‚Äôt just a new number; it's a result of a decision. These aren't CRUD operations. They're **business moments**.

Every change is an event. Every event is immutable. Every event is meaningful. That sequence of decisions becomes your system's memory ‚Äî a reliable, replayable timeline.

This unlocks real-world clarity:
- Why is this account frozen?
- What name was on the account during the last withdrawal?
- When was the transition from pending to closed?

You don't query five tables. You read the timeline.

---

### How the System Works Internally

When a command like `CloseAccount` is issued, the system applies domain logic and emits a new event: `AccountClosed`. That event is persisted to an **append-only event store**, applied to the aggregate to evolve its state, and optionally broadcasted to other services.

The aggregate never stores the final state directly. It‚Äôs rebuilt by replaying its event history. This guarantees consistency, traceability, and recovery. The behavior is deterministic.

---

### Read Models and Projections

You don‚Äôt query the event log directly for reads. Instead, **projections** build dedicated, query-optimized views of the event stream. These projections are typically stored in their own databases or indexes (e.g., Postgres, MongoDB, Elasticsearch).

Projections decouple the read model from the domain model, enabling:
- Custom read views per use case
- Fast query performance
- Safe rebuilds via event replay

They also enable system resilience: if a read model is corrupted, replay the stream and rebuild.

---

### When to Use Event Sourcing (And When Not To)

Event Sourcing isn't for everything. It‚Äôs **not** a default choice. It's a strategic one.

It fits well when:
- The domain has **complex workflows** or lifecycles
- You need **auditability**, **traceability**, or **replayability**
- You require **time-travel debugging** or rollback
- Events represent **real-world business facts**

It‚Äôs overengineering when:
- The domain is CRUD-heavy and flat
- Events are synthetic and meaningless
- The cost of projections outweighs the benefits
- You're doing it "because it looks cool"

Use Event Sourcing when behavior matters more than structure. Skip it when you just need data.

---

### Event Sourcing vs. Event-Driven Architecture

These two often get mixed up. They‚Äôre related, but serve different purposes:

- **Event Sourcing** is a *persistence and modeling* strategy. It lives **inside** a service. You store events as the canonical source of truth.
- **Event-Driven Architecture** is a *communication* strategy. It connects services using published/subscribed events.

You can do EDA without sourcing. You can source without broadcasting. Many systems use both together.

---

### Event Store: The Source of Truth

The **Event Store** is not just a place to keep logs. It's your primary database. It's append-only, immutable, and designed for **replayable truth**.

Key properties of an event store:
- Ordered, immutable streams per aggregate
- Idempotent event appends
- Support for snapshots and schema evolution
- Replayability for projections and aggregate rebuilding

This is not Kafka. This is not a side table. This is your **system of record**.

The event store enables:
- State reconstruction from first principles
- Infinite audit history
- Safe recovery from corruption or bugs
- Projections and derived data that are always replaceable

But it comes with real trade-offs.

---

### Trade-Offs and Costs

Adopting Event Sourcing and an Event Store means taking on:
- Higher complexity in modeling and infrastructure
- Managing event schema evolution
- Maintaining projections and replay pipelines
- Snapshot strategies for performance
- More work upfront in understanding domain logic

You also write your domain logic twice:
- **Aggregates** focus on consistency, enforcing business rules during commands
- **Projections** focus on efficient querying, tailored for read models

This duplication is intentional. It reflects **CQRS**: the separation between commands (writes) and queries (reads).

It's not DRY. It's **explicit responsibility separation**.

---

